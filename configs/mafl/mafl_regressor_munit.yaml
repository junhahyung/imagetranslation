# Copyright (C) 2017 NVIDIA Corporation.  All rights reserved.
# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

model: MUNIT
loss: regression_loss
metrics: [inter_ocular_error]
epoch: 300
keypoint_regressor:
    type: IntermediateKeypointPredictor
    args:
        input_channel: 256 
        num_annotated_points: 5
        num_intermediate_points: 50
        softargmax_mul: 100

optimizer:
    type: Adam
    args:
        lr: 0.0001
        weight_decay: 0.0005
        amsgrad: true

lr_scheduler:
    type: MultiStepLR
    args:
        milestones: [100, 250]
        gamma: 0.1

# logger options
image_save_iter: 10000        # How often do you want to save output images during training
image_display_iter: 500       # How often do you want to display output images during training
display_size: 16              # How many images do you want to display each time
snapshot_save_iter: 10000     # How often do you want to save trained models
log_iter: 10                  # How often do you want to log the training stats

# optimization options
max_iter: 1000000             # maximum number of training iterations
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.00001                 # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 100000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
gan_w: 1                      # weight of adversarial loss
recon_x_w: 10                 # weight of image reconstruction loss
recon_s_w: 1                  # weight of style reconstruction loss
recon_c_w: 1                  # weight of content reconstruction loss
recon_x_cyc_w: 0              # weight of explicit style augmented cycle consistency loss
vgg_w: 0                      # weight of domain-invariant perceptual loss
input_dim: 3 # input dimension channel


# model options
gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 256                # number of filters in MLP
  style_dim: 8                # length of style code
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 4                    # number of residual blocks in content encoder/decoder
  pad_type: reflect           # padding type [zero/reflect]

data:
    dataset: MAFLAligned
    task: Munit # for which task data is served (Munit, None, ...)
    root: /home/junhahyung/unsuplandmark/data/celeba 
    use_keypoints: True # return keypoints in dataloader, True or False
    batch_size: 32
    default_h: 218 # default height
    default_w: 178 # default width
    use_hq_ims: True
    val_split: celeba
    val_size: 2000
    transform: 
        CenterCrop: [256, 256]  # [h, w] or scalar
        Resize: 256 # scalar

## MAFL
checkpoint_dir: ./outputs/celeba4mafl_munit/checkpoints
encoder_domain: a
unsup_landmarks: 50
num_landmarks: 5
